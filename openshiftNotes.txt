Notes on Openshift bare met
#From Operating Openshift book chapter 6

###################
#How to install
#################

https://www.youtube.com/watch?v=xQcFEib3hR8&t=461s

###########
##kubeadmin
##xA6Y4-xDHTH-DT7Yq-Ax2aG
## P2z9R-vPvyz-5B2IP-Ds9W2
## XPER3-EKS9P-6Fu7y-CGKDs
## xUbh4-WD8zf-hQ6vx-ZBD56
## yDuex-PYDBp-z2GoS-VnVgs
###########
#How to port forward the Prometheous instance route

https://blog.cubieserver.de/2023/debugging-prometheus-on-openshift-the-hard-way/

# You need to login as developer and not administrator to login in Openshift

oc login --token=sha256~Xtb2gv8OzPh65bSi9g8agnlnUShR-Jukrwd887WTJ84 --server=https://api.yogi-ocp.yogi-ocp.com:6443

2. Prometheous:


#I think prometheous comes installed, if not create a configmap:

monitoringPersistence.yaml


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: 10d
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
oc apply -f monitoringPersistence.yaml

# Describe all the pods in monitoring namespace:
oc -n openshift-monitoring get pods

#Describe the pod promethous
oc describe pod prometheus-k8s-0 -n openshift-monitoring

#It has 6 containors in single pod, Prometheus is only listening on localhost (making it inaccessible from outside the pod) and only particular features and routes are exposed with #various RBAC proxies. Check the output of oc -n openshift-monitoring describe sts/prometheus-k8s for all the details.
#This means we can not just connect to the Prometheus pod, but instead we first need to make Prometheus server endpoint available externally.

oc -n openshift-monitoring exec pod/prometheus-k8s-0 -- socat tcp-l:5050,fork,reuseaddr tcp:127.0.0.1:9090 &

#This command will forward connections received at 0.0.0.0:5050 to 127.0.0.1:9090 - which is the port the (unauthenticated) Prometheus web interface is listening on.
# then we need to connect to port 5050 in the pod from our local machin

oc port-forward -n openshift-monitoring pod/prometheus-k8s-0 5050:5050

# UI is available at:

http://localhost:5050/graph


####################################################################
###Create a rule called up: all the service which are in state "up"#
####################################################################


ourRule.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: up
spec:
  groups:
  - name: up.rules
    rules:
    - expr: sum by (service) (up)
      record: services:up:sum
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
oc apply -f ourRule.yaml

#Now if you go to http://localhost:5050/graph click on table and type "up" in the search bar and it will show all the service in state up
# the PromQl follows this style: name{ label="labelValue" }:value. Like this: 

#up{apiserver="kube-apiserver",
#endpoint="https",
#instance="192.168.126.11:6443",
#job="apiserver",
#namespace="default",
#service="kubernetes"}:1

#This means the rule name is up and service anme is apiserver etc vale is 1 which is running

######################################
##Does not work#######################
######################################

#Up rule promethusRule does not seem to be visible in local host url. Something might be wrong. I will try to send alert to remote alert processing systems like pagerDuty or slack
#1. create an alert:


oc apply -f ourAlert.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: yogi
  namespace: openshift-monitoring
  labels:
    role: prometheus-rulefiles
spec:
  groups:
  - name: yogi.rules
    rules:
    - expr: sum by (service) (up)
      record: services:up:sum
    - expr: services:up:sum < 1
      alert: notUp
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: Service is not up

>>>>>>>>>>>>>>>>>>>>>>>>>>
#This is an a promethusRule kind with name yogi. However the name of the alert is notUp. So we need to send this notUp name to pager duty.
# This Rule/Alert says expr: sum by (service) (up) The expression to evaluate, which sums the up metrics by service. record: stores the result of expr.
# second rule: expr: services:up:sum < 1 1 is up and 0 is down, So this expression is 0 means down then fire an alert. 
#In general sum group by service and if that is smaller then 1 i.e. 0 the fire an alert. 
#There are two rules one is record rule and other is alert rule. So we will record this rule and fire an alert based on that rule.  we will name this alert as notUp.
# fire it if the record rule is smaller then 1 for more then 10 mins. put some lable: critical etc.

##############
#pager durty 
#############
Now go to pager duty: new service -> name of the servce -> integration (choose promethus) -> find the servcie_key and url

#put that in the alertmanager.yaml, this below command will get the secret in alertmanager config which has base64 encoded  rules, receievers, routes etc
# This will create a local file alertmanager.yaml you need to put the url and service key here:
oc -n openshift-monitoring get secret alertmanager-main --template='{{index .data "alertmanager.yaml" }}' | base64 --decode | tee alertmanager.yaml

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
"inhibit_rules":
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = critical"
  "target_matchers":
  - "severity =~ warning|info"
- "equal":
  - "namespace"
  - "alertname"
  "source_matchers":
  - "severity = warning"
  "target_matchers":
  - "severity = info"
"receivers":
- "name": "Default"
- "name": "Watchdog"
- "name": "Critical"
- "name": "OpenShiftDefault"
  "pagerduty_configs":
  - "service_key": "874883fc7d6e4505c14128e2218c8eba"
    "url": "https://events.eu.pagerduty.com/generic/2010-04-15/create_event.json"
"route":
  "group_by":
  - "namespace"
  "group_interval": "5m"
  "group_wait": "30s"
  "receiver": "Default"
  "repeat_interval": "12h"
  "routes":
  - "matchers":
    - "alertname = Watchdog"
    "receiver": "Watchdog"
  - "matchers":
    - "severity = critical"
    "receiver": "Critical"
  - "receiver": "OpenShiftDefault"
    "match":
      "alertname": "notUp"

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# newely added:

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
- "name": "OpenShiftDefault"
  "pagerduty_configs":
  - "service_key": "874883fc7d6e4505c14128e2218c8eba"
    "url": "https://events.eu.pagerduty.com/generic/2010-04-15/create_event.json"


  - "receiver": "OpenShiftDefault"
    "match":
      "alertname": "notUp"

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

oc apply -f alertmanger.yaml


#Now go to url of openshift -> cluster settings ->  configuration -> Alertmanager -> Receiver(see the OpenshfitDefault - pager)
# when it fires the pagerduty service directory will show the alerts and it will also call


################################################
#######User workload monitoring#################
################################################


#Two types of monitoring:

#1 service monitoring: which takes care of sytsem service
#2. workload monitoring: takes care of user workload pods etc

#create user workload monitoring pods:

user-workload-monitoring.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

oc apply -f user-workload-monitoring.yaml

oc -n openshift-user-workload-monitoring get pod

#3 pods will spin up: prometheus-operator-8fc5898cc-rs99m, pormethus-user-workload, thanos ruler


#create namespace where service will be deployed to ve scraped via proemtheus


user-namespace.yaml
<<<<<<<<<<<<<<<<<
apiVersion: v1
kind: Namespace
metadata:
  name: ns1
>>>>>>>>>>>>>>>

oc apply -f user-namespace.yaml


user-deployment.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-example-app
  template:
    metadata:
      labels:
        app: prometheus-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.0
        imagePullPolicy: IfNotPresent
        name: prometheus-example-app
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
oc apply -f user-deployment.yaml

user-service.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
      name: web
  selector:
    app: prometheus-example-app
  type: ClusterIP
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

oc apply -f user-service.yaml
#this is the service which will be scraped via promethus. Generaly services are scrapped and not th eunderlying instances or pods, as the pods can vary in numbers and dying
# and popping out, so better is to scrap the service they expose. This service exposes a port 8080 named web, we will scrap this port.

# now we need ServiceMonitor CRD which will give info about which service to scrap which port to scrap and which time interval etc:

user-serviceMonitor.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus-example-monitor
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
    - interval: 30s
      port: web
      scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
oc apply -f user-serviceMonitor.yaml

#it says whcih app: prometheus-example-app to scrap, which port: web, interval: 30s, etc

#now create prometheusRule:

user-promethusRuke.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: deleteup
  namespace: ns1
spec:
  groups:
    - name: up.rules
      rules:
      - alert: deleteNotUp
        expr: sum by (service) (up{job="prometheus-example-app"}) ==1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Service is not up
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# here name is deleteup, but the name of the alert is deleteNotUp. So the alert will show up in obeserve tab in Openshift observe -> Alerts -> AlertRule (or something like that). You will see this and you will see the expression: sum service (by) (up{job="prometheus-example-app"}) ==1 in that tab also, this means that if the service  called prometheus-example-app is scrapped by metric called "up" for more then 1m fire it up and tag it as critical. This is just for testing. You can set it up to 0. 

# Now you need alert manager.yaml:

user-alertManager.yaml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: example-routing
  namespace: ns1
spec:
  route:
    receiver: default
    groupBy: [job]
  receivers:
    - name: default
      pagerdutyConfigs:
        - url: 'https://events.eu.pagerduty.com/generic/2010-04-15/create_event.json'
          serviceKey:
            name: 'pagerduty-config'
            key: 'serviceKey'
---
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: pagerduty-config
  namespace: ns1
data:
  serviceKey: ODc0ODgzZmM3ZDZlNDUwNWMxNDEyOGUyMjE4YzhlYmE=
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#It has alertmangerConfig and secret to menstion the url and key of pagerDuty. This secretkey has to be base64encoded

##################
## SLO############
##################

#SLI/SLO: service level intdicator/ service level objective

#its is a KPI on how is my servcie is performing. If you have 3 master node and 3 workder node cluster. YOu can have hundreds of pods. But yOu really want to monitor those pods?
#SO to get the best of monitoring, monitor service instead.

#So imagine you want your goland servcie to give you 200 status code 99% of the time. Now imagine that this 99% frops to 80% now you also would have to guess if users request droped #or there was really issues in the service? So the better SLI is how much error or how much non 200 status code the service is producing along with the tiem it is taking. This is #better indicator and is called SLI.

#Stage 1: all request which do not succeed:

apiserver_request_total{code=~"5..",job="apiserver"}

#output:
<<<<<<<<<<<<<<<<<<<<<<<
Name, apiserver, code, component, endpoint, group, instance, job, namespace, prometheus, resource, scope, service, subresource, system_client, verb, version, Value


apiserver_request_total	kube-apiserver	500	apiserver	https	monitoring.coreos.com	192.168.122.88:6443	apiserver	default	openshift-monitoring/k8s	alertmanagerconfigs	cluster	kubernetes		cluster-policy-controller	LIST	v1beta1	3
apiserver_request_total	kube-apiserver	500	apiserver	https	monitoring.coreos.com	192.168.122.88:6443	apiserver	default	openshift-monitoring/k8s	alertmanagerconfigs	cluster	kubernetes		kube-controller-manager	LIST	v1beta1	8


>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# apiserver_request_total this is the metric whic apiserver exposes which we can scrap. API server receives all teh necessary requests over https from all other resources, be it kubectl, CRDs, pods, deployments etc. So in this query we are querying prometheus to give all the requests ended with status code 500 for the group of pods (label=job=apiserver). Job is group of pods for exmample labeled as apiserver.  In the last coloumn, its written value which is 3 and 8 for both rows. That means 3 requests and 8 requests.

#similarly, this belw query gives all the requests made to apiserver in last 5 minutes: so total rows 10 like this: 23 @1722763258.162

apiserver_request_total{job="apiserver"}[5m]

<<<<<<<<<<<<<<<<<<<<<<<
apiserver_request_total	kube-apiserver	0	apiserver		https	apiextensions.k8s.io	192.168.122.88:6443	apiserver	default	openshift-monitoring/k8s	customresourcedefinitions	cluster	kubernetes			WATCH	v1	
23 @1722763258.162
23 @1722763288.162
23 @1722763318.162
24 @1722763348.162
24 @1722763378.162
25 @1722763408.162
25 @1722763438.162
25 @1722763468.162
25 @1722763498.162
26 @1722763528.162
>>>>>>>>>>>>>>>>>>>>>>
#here number of requests handled by api server was 23 at time stamp of 1722763258.162. The data who made reuqest is no available by this apiserver-request-total metrix. 

# moving ahead to find the rate =:
rate(apiserver_request_total{job="apiserver"}[5m])

<<<<<<<<<<<<<
kube-apiserver	0	apiserver		https	apiextensions.k8s.io	192.168.122.88:6443	apiserver	default	openshift-monitoring/k8s	customresourcedefinitions	cluster	kubernetes			WATCH	v1	0.011111111111111112

>>>>>>>>>>>>>>>

This give steh rate 0.011111111111111112 per sec in the last 5 minutes requests made to apiserver

# Lets sum this rate:

sum(rate(apiserver_request_total{job="apiserver"}[5m]))

<<<<<<<<<<<<
80.74814814814805
>>>>>>>>>

#This means average rate of requests in teh last 5 minutes is 80 to apiserver

# No do this:

sum(rate(apiserver_request_total{code=~"5..", job="apiserver"}[5m])) /
sum(rate(apiserver_request_total{job="apiserver"}[5m]))

#This calculates to total number of failed requests in the last 5 minutes to the total number of reuquetss made in last 5 minutes.
#For me it is giving 0 fro some reason. Chtgpt says there were no failed requests in last 5 minutes, nad that is true no requests to apiserver failed in teh last 5, 10 mins.
# this is because, apiserer fails rairly. probably. But 

when i tried this: 

sum(rate(apiserver_request_total{code=~"2..", job="apiserver"}[5m])) /sum(rate(apiserver_request_total{job="apiserver"}[5m]))

it gave me 0.97 something, so this is passing percent to requests, i guess

# sending alert when the this error to total reuqest ratio is greater then 0.01, in prometheus rule as we did previously:

- alert: HighErrorRate
expr: sum(rate(apiserver_request_total{code=~"5..", job="apiserver"}[5m])) /
sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.01
for: 1h

# and send it to pagerduty
# Threshold (> 0.01): This checks if the proportion of 5xx errors is greater than 1% (0.01). If the proportion of 5xx errors is more than 1%, the expression evaluates to true
# for: 1h: This specifies that the condition must be true for a continuous duration of 1 hour before the alert is triggered. This is used to avoid triggering alerts for brief spikes  # or transient issues. The alert will only fire if the error rate exceeds 1% continuously for the past hour.#####


####################################################################
########## Elasticsearch (ECK) Operator
####################################################################

#ClusterLogging consists of two main components: openshift-logging and an
#elasticsearch-operator. They live in separate namespaces, and it is very important
#to be careful with the naming
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"


apiVersion: v1
kind: Namespace
metadata:
  name: openshift-operators-redhat
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-monitoring: "true"


##This is probelm from ythe book, the book installed the ECK using subscription and it failed. I had to do work wround to delete the subscription and csv and jobs realted to
## operator installation. Try to install it using UI. It sis better. Refer the doc: elasticsearchSubscription in the same place this doc resides
## once this is installed search like this:

oc get csv -n openshift-logging

NAME                                           DISPLAY                        VERSION   REPLACES                                       PHASE
cluster-logging.v5.9.4                         Red Hat OpenShift Logging      5.9.4     cluster-logging.v5.9.3                         Succeeded
elasticsearch-eck-operator-certified.v2.13.0   Elasticsearch (ECK) Operator   2.13.0    elasticsearch-eck-operator-certified.v2.12.1

#both cluster-logging and elasticsearch-eck-operator is necessary.

####################################
#Automating tasks#####################
####################################

This will get the dns of the highscore 
DOMAIN=$(oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}') 






#####################
#pv and pvcd locally#
#####################

oc apply -f pclocal.yaml

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 5Gi  # Size of the volume
  accessModes:
    - ReadWriteOnce  # The PV can be mounted as read-write by a single node
  hostPath:
    path: /mnt/data  # Path on the node where the data will be stored
  persistentVolumeReclaimPolicy: Retain  # What to do with the PV after the claim is deleted
  storageClassName: manual  # Storage class associated with the PV (can be any name)


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


oc app;ly -f pvclocal.yaml

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc  # Name of the PVC
  namespace: arcade  # Replace with your actual namespace
spec:
  accessModes:
    - ReadWriteOnce  # Must match the access mode of the PV
  resources:
    requests:
      storage: 2Gi  # Must match the size of the PV
  storageClassName: manual 

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

oc get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
local-pv   5Gi        RWO            Retain           Bound    arcade/local-pvc   manual         <unset>                          56m

oc get pvc
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
local-pvc   Bound    local-pv   5Gi        RWO            manual         <unset>                 43m

#storage class: amnual means bound pvc to pv which has manual

dhkshdlkdshlkhskldhsklh lksjdkl
i MA yogi
SHALU IS MY before
chandra is my wife
tarun was my friend